{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "\"\"\"\n",
    "In this section, we will look at how we can learn a word embedding while fitting a neural network on a \n",
    "text classification problem.\n",
    "\n",
    "We will define a small problem where we have 10 text documents, each with a comment about a piece of work a \n",
    "student submitted. Each text document is classified as positive “1” or negative “0”. This is a simple sentiment \n",
    "analysis problem.\n",
    "\n",
    "First, we will define the documents and their class labels.\n",
    "\"\"\"\n",
    "\n",
    "# define documents\n",
    "docs = ['Well done!',\n",
    "\t\t'Good work',\n",
    "\t\t'Great effort',\n",
    "\t\t'nice work',\n",
    "\t\t'Excellent!',\n",
    "\t\t'Weak',\n",
    "\t\t'Poor effort!',\n",
    "\t\t'not good',\n",
    "\t\t'poor work',\n",
    "\t\t'Could have done better.']\n",
    "# define class labels\n",
    "labels = array([1,1,1,1,1,0,0,0,0,0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 42], [41, 15], [28, 45], [26, 15], [2], [24], [15, 45], [7, 41], [15, 15], [33, 21, 42, 34]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Next, we can integer encode each document. This means that as input the Embedding layer will have sequences \n",
    "of integers. We could experiment with other more sophisticated bag of word model encoding like counts or TF-IDF.\n",
    "\n",
    "Keras provides the one_hot() function that creates a hash of each word as an efficient integer encoding. \n",
    "We will estimate the vocabulary size of 50, which is much larger than needed to reduce the probability of \n",
    "collisions from the hash function.\n",
    "\"\"\"\n",
    "\n",
    "# integer encode the documents\n",
    "vocab_size = 50\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
    "print(encoded_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2 42  0  0]\n",
      " [41 15  0  0]\n",
      " [28 45  0  0]\n",
      " [26 15  0  0]\n",
      " [ 2  0  0  0]\n",
      " [24  0  0  0]\n",
      " [15 45  0  0]\n",
      " [ 7 41  0  0]\n",
      " [15 15  0  0]\n",
      " [33 21 42 34]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The sequences have different lengths and Keras prefers inputs to be vectorized and all inputs to have \n",
    "the same length. We will pad all input sequences to have the length of 4. Again, we can do this with a built \n",
    "in Keras function, in this case the pad_sequences() function.\n",
    "\"\"\"\n",
    "\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 4, 8)              400       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 433\n",
      "Trainable params: 433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We are now ready to define our Embedding layer as part of our neural network model.\n",
    "\n",
    "The Embedding has a vocabulary of 50 and an input length of 4. We will choose a small embedding \n",
    "space of 8 dimensions.\n",
    "\n",
    "The model is a simple binary classification model. Importantly, the output from the Embedding \n",
    "layer will be 4 vectors of 8 dimensions each, one for each word. We flatten this to a one 32-element \n",
    "vector to pass on to the Dense output layer.\n",
    "\"\"\"\n",
    "\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# summarize the model\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 89.999998\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
